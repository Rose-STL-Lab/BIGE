python3 train_vq.py --batch-size 128 --lr 2e-4 --total-iter 300000 --lr-scheduler 200000 --nb-code 512 --down-t 2 --depth 3 --dilation-growth-rate 3 --out-dir output --dataname mcs --vq-act relu --quantizer ema_reset --loss-vel 0.5 --recons-loss l1_smooth --exp-name VQVAE3 --resume-pth /home/ubuntu/data/T2M-GPT/output/VQVAE3/300000.pth
